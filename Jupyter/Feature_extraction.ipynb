{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230eba3b-6067-48a8-a05c-834638467a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests python-whois dnspython pandas scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf39521c-a3ee-4161-b338-bb2e34fc14c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipwhois in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (1.2.0)\n",
      "Requirement already satisfied: dnspython<=2.0.0 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from ipwhois) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipwhois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a121ea-3cad-4aa7-b91c-8cfa4b27456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2a5c43-288b-4d64-b9fb-748f15a91c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tldextract in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (5.1.2)\n",
      "Requirement already satisfied: idna in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from tldextract) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from tldextract) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from tldextract) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from tldextract) (3.15.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.1.0->tldextract) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win10\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.1.0->tldextract) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5181fd-d91e-4d01-98c1-99b11419ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features with Names: {'directory_length': 0, 'time_domain_activation': 6, 'asn_ip': 0, 'time_response': 0.207, 'length_url': 20, 'ttl_hostname': 0, 'qty_dot_domain': 1, 'time_domain_expiration': -1, 'qty_nameservers': 0, 'domain_length': 11, 'qty_slash_url': 3, 'qty_mx_servers': 0, 'qty_ip_resolved': 0, 'qty_vowels_domain': 3, 'qty_hyphen_directory': -1, 'qty_redirects': 0, 'file_length': -1, 'qty_dot_url': 0, 'qty_slash_directory': 1, 'tls_ssl_certificate': 0}\n",
      "\n",
      "The URL: https://st-join.com/ is Phishing!!\n"
     ]
    }
   ],
   "source": [
    "import whois\n",
    "import os\n",
    "import dns.resolver\n",
    "import requests\n",
    "import time\n",
    "import socket\n",
    "import ssl\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features(url):\n",
    "    features = []\n",
    "    \n",
    "    # Feature 1: directory_length\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        url_path = parsed_url.path.rsplit('/', 1)[0]\n",
    "        features.append(len(url_path) if url_path else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 2: time_domain_activation\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "        creation_date = domain_info.creation_date\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        features.append((datetime.now() - creation_date).days)\n",
    "    except:\n",
    "        features.append(-1)\n",
    "\n",
    "    # Feature 3: asn_ip\n",
    "    try:\n",
    "        ip = socket.gethostbyname(urlparse(url).netloc)\n",
    "        asn = requests.get(f\"https://ipapi.co/{ip}/asn/\").text.strip()\n",
    "        features.append(asn.strip('AS')) if asn else features.append(0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 4: time_response\n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = requests.get(url, timeout=5)\n",
    "        end = time.time()\n",
    "        features.append(end - start)\n",
    "    except:\n",
    "        features.append(0.207 if not None else 0.207)\n",
    "\n",
    "    # Feature 5: length_url\n",
    "    try:\n",
    "        features.append(len(url))\n",
    "    except:\n",
    "        features.append(0)\n",
    "    \n",
    "    # Feature 6: ttl_hostname\n",
    "    try:\n",
    "        domain = tldextract.extract(url).registered_domain\n",
    "        ttl = dns.resolver.resolve(domain, 'NS').rrset.ttl\n",
    "        features.append(ttl)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 7: qty_dot_domain\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        features.append(domain.count('.') if '.' in domain else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 8: time_domain_expiration\n",
    "    try:\n",
    "        expiration_date = whois.whois(url).expiration_date\n",
    "        today = datetime.now()\n",
    "        features.append((expiration_date - today).days)\n",
    "    except:\n",
    "        features.append(-1)\n",
    "\n",
    "    # Feature 9: qty_nameservers\n",
    "    try:\n",
    "        domain = tldextract.extract(url).registered_domain\n",
    "        ns_list = dns.resolver.resolve(domain, 'NS')\n",
    "        features.append(len(ns_list))\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 10: domain_length\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        features.append(len(domain) if domain else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 11: qty_slash_url\n",
    "    try:\n",
    "        features.append(url.count('/') if '/' in url else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 12: qty_mx_servers\n",
    "    try:\n",
    "        domain = tldextract.extract(url).registered_domain\n",
    "        mx_list = dns.resolver.resolve(domain, 'MX')\n",
    "        features.append(len(mx_list))\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 13: qty_hyphen_directory\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        url_path = parsed_url.path.rsplit('/', 1)[0]\n",
    "        features.append(url_path.count('-') if '-' in url_path else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 14: qty_vowels_domain\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        vowels = set(['a', 'e', 'i', 'o', 'u', 'A','E','I','O','U'])\n",
    "        qty_vowels = sum(1 for c in domain if c in vowels)\n",
    "        features.append(qty_vowels)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 15: qty_ip_resolved\n",
    "    try:\n",
    "        ip_list = socket.getaddrinfo(urlparse(url).netloc, None)\n",
    "        features.append(len(ip_list))\n",
    "    except:\n",
    "        features.append(-1)\n",
    "\n",
    "    # Feature 16: file_length\n",
    "    try:\n",
    "        file_name = os.path.basename(urlparse(url).path)\n",
    "        features.append(len(file_name) if file_name else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 17: qty_redirects\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        features.append(len(response.history))\n",
    "    except:\n",
    "        features.append(-1)\n",
    "\n",
    "    # Feature 18: qty_slash_directory\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        url_path = parsed_url.path.rsplit('/', 1)[0]\n",
    "        features.append(url_path.count('/') if '/' in url_path else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 19: qty_dot_url\n",
    "    try:\n",
    "        features.append(url.count('.') if '.' in url else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    # Feature 20: qty_dot_file\n",
    "    try:\n",
    "        file_name = os.path.basename(urlparse(url).path)\n",
    "        features.append(file_name.count('.') if '.' in file_name else 0)\n",
    "    except:\n",
    "        features.append(0)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Load the saved model and feature names\n",
    "with open('../Model/random_forest_model.pkl', 'rb') as model_file:\n",
    "    model = joblib.load(model_file)\n",
    "\n",
    "with open('../Model/feature_names.pkl', 'rb') as feature_names_file:\n",
    "    feature_names = joblib.load(feature_names_file)\n",
    "\n",
    "# Test the function with a sample URL\n",
    "sample_url = \"https://st-join.com/\"\n",
    "extracted_features = extract_features(sample_url)\n",
    "\n",
    "# Create a dictionary of feature names and their values\n",
    "features_dict = dict(zip(feature_names, extracted_features))\n",
    "\n",
    "# Print the extracted features with their names\n",
    "print(\"Extracted Features with Names:\", features_dict)\n",
    "\n",
    "# Convert the extracted features to a DataFrame with the correct feature names\n",
    "features_df = pd.DataFrame([extracted_features], columns=feature_names)\n",
    "\n",
    "# Predict if the URL is legitimate or phishing\n",
    "prediction = model.predict(features_df)\n",
    "\n",
    "if prediction == 1:\n",
    "    print(f\"\\nThe URL: {sample_url} is Phishing!!\")\n",
    "else:\n",
    "    print(f\"\\nThe URL: {sample_url} is Legitimate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ec207-f4fe-4eb6-b686-063ae117dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
